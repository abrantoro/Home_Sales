The purpose of this Module is to use APACHE SPARKS and SPARKSSQL to view larger dataframes and home sales and compute key housing metrics efficiently. 

# Summary 
While date built partitioning enabled scalable and structured data analysis, caching greatly enhanced query performance. All things considered, the project demonstrated SparkSQL's strength and adaptability in managing complex data processing jobs.
The SparkSQL Analysis showed how to evaluate a sizable dataset of home sales using PySpark and SQL-style queries. The homework demonstrated important techniques like building queries to filter data, caching tables to improve performance, and using Parquet to divide data for effective storage and retrieval.
